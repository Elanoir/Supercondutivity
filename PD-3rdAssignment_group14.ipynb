{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a7397a",
   "metadata": {},
   "source": [
    "## Prospecção de Dados 2022/2023\n",
    "### Mestrado em Ciência de Dados | Third Home Assignment\n",
    "\n",
    "#### Group 14: Tiago Gil, Nº59453  |  Nuno Lopes, Nº59461  |  Daniela Moutinho, Nº57064\n",
    "\n",
    "#### Task: Build Unsupervised Learning Models (Clustering)\n",
    "Dataset: UCI Supercoductivity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e356196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.metrics.cluster import silhouette_score, silhouette_samples, calinski_harabasz_score, homogeneity_score, completeness_score, v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f71e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load both base datasets\n",
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_unique = pd.read_csv('unique_m.csv')\n",
    "\n",
    "#ChatGPT generated conversion method but slightly adapted from original output\n",
    "def get_temp_class(temp):\n",
    "    if temp < 1.0:\n",
    "        return \"VeryLow\"\n",
    "    elif temp < 5.0:\n",
    "        return \"Low\"\n",
    "    elif temp < 20.0:\n",
    "        return \"Medium\"\n",
    "    elif temp < 100.0:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"VeryHigh\"\n",
    "    \n",
    "data_train['temp_class'] = data_train['critical_temp'].apply(get_temp_class)\n",
    "\n",
    "#Separate the dependent variable as our target. This value is the same in both datasets and same order\n",
    "y_classes = data_train.values[:,82]\n",
    "\n",
    "#Drop the dependent variable and the identifier variables\n",
    "data_train.drop(columns=[\"critical_temp\", \"temp_class\"], inplace=True)\n",
    "data_unique.drop(columns=[\"critical_temp\", \"material\"], inplace=True)\n",
    "\n",
    "#Scale\n",
    "train_scaled = StandardScaler().fit_transform(data_train)\n",
    "unique_scaled = StandardScaler().fit_transform(data_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07d781",
   "metadata": {},
   "source": [
    "## Objective 1: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb886c",
   "metadata": {},
   "source": [
    "Before clustering we could have performed some dimension reduction to reduce noisy and irrelevant information, as the size of the dataset, in an attempt to reduce computational effort. However, since that was not asked, and due to very limited space, that task was not performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e83283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering(X, n_clusters, method, eps_, min_samples_):\n",
    "    if method == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        labels = model.fit_predict(X)\n",
    "        inertia = model.inertia_\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "        return labels, inertia, silhouette, ch_score\n",
    "           \n",
    "    elif method == 'hca':\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\")\n",
    "        labels = model.fit_predict(X)\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "        return labels, None, silhouette, ch_score\n",
    "    \n",
    "    elif method == 'dbscan':\n",
    "        model = DBSCAN(eps=eps_, min_samples=min_samples_)\n",
    "        labels = model.fit_predict(X)\n",
    "        n_clusters = len(np.unique(labels)) - 1\n",
    "        if n_clusters == 0:\n",
    "            return None, None, None, None\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "        return labels, None, silhouette, ch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6d5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Method:  kmeans\n",
      "Clusters: [2, 3, 4, 5, 6, 7, 8]\n",
      "Duration:  [8.236, 8.862, 9.432, 8.855, 10.655, 10.985, 12.474]\n",
      "Labels:  [array([0, 0, 0, ..., 1, 1, 1]), array([0, 0, 0, ..., 1, 1, 2]), array([1, 1, 1, ..., 0, 0, 3]), array([1, 1, 1, ..., 2, 2, 0]), array([0, 0, 0, ..., 2, 2, 4]), array([3, 3, 3, ..., 1, 1, 6]), array([6, 6, 6, ..., 7, 7, 4])]\n",
      "Inertia:  [1173756.429, 1039073.838, 961811.049, 898430.53, 842678.088, 788053.678, 751401.145]\n",
      "Silhouette score:  [0.347, 0.314, 0.304, 0.256, 0.263, 0.266, 0.268]\n",
      "Calinski-Harabasz score:  [9936.174, 6989.615, 5603.075, 4873.466, 4437.802, 4199.884, 3923.436]\n",
      "\n",
      "Method:  hca\n",
      "Clusters: [2, 3, 4, 5, 6, 7, 8]\n",
      "Duration:  [56.665, 40.177, 39.797, 48.168, 39.47, 40.121, 39.522]\n",
      "Labels:  [array([1, 1, 1, ..., 0, 0, 0], dtype=int64), array([0, 0, 0, ..., 2, 2, 1], dtype=int64), array([1, 1, 1, ..., 2, 2, 0], dtype=int64), array([0, 0, 0, ..., 2, 2, 1], dtype=int64), array([3, 3, 3, ..., 2, 2, 0], dtype=int64), array([3, 3, 3, ..., 0, 0, 2], dtype=int64), array([3, 3, 3, ..., 2, 2, 0], dtype=int64)]\n",
      "Inertia:  [None, None, None, None, None, None, None]\n",
      "Silhouette score:  [0.333, 0.298, 0.273, 0.282, 0.244, 0.248, 0.255]\n",
      "Calinski-Harabasz score:  [9125.608, 6301.595, 5114.066, 4540.792, 4167.613, 3876.374, 3675.06]\n",
      "\n",
      "Method:  kmeans\n",
      "Clusters: [2, 3, 4, 5, 6, 7, 8]\n",
      "Duration:  [7.492, 7.829, 8.044, 7.856, 7.951, 7.846, 7.748]\n",
      "Labels:  [array([1, 1, 1, ..., 1, 1, 1]), array([1, 1, 1, ..., 1, 1, 1]), array([1, 1, 1, ..., 1, 1, 1]), array([1, 1, 1, ..., 1, 1, 1]), array([4, 4, 4, ..., 4, 4, 4]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0])]\n",
      "Inertia:  [1594289.884, 1568723.042, 1522185.528, 1518505.556, 1481400.857, 1447320.673, 1412585.115]\n",
      "Silhouette score:  [0.701, 0.702, 0.078, 0.073, 0.114, 0.087, 0.095]\n",
      "Calinski-Harabasz score:  [572.917, 464.36, 535.672, 415.588, 447.267, 464.9, 482.932]\n",
      "\n",
      "Method:  hca\n",
      "Clusters: [2, 3, 4, 5, 6, 7, 8]\n",
      "Duration:  [56.411, 46.782, 46.218, 49.151, 47.693, 54.098, 54.261]\n",
      "Labels:  [array([0, 0, 0, ..., 0, 0, 0], dtype=int64), array([0, 0, 0, ..., 0, 0, 0], dtype=int64), array([0, 0, 0, ..., 0, 0, 0], dtype=int64), array([0, 0, 0, ..., 0, 0, 0], dtype=int64), array([2, 2, 2, ..., 2, 2, 2], dtype=int64), array([2, 2, 2, ..., 2, 2, 2], dtype=int64), array([0, 0, 0, ..., 0, 0, 0], dtype=int64)]\n",
      "Inertia:  [None, None, None, None, None, None, None]\n",
      "Silhouette score:  [0.701, 0.06, 0.066, 0.07, 0.083, 0.083, 0.084]\n",
      "Calinski-Harabasz score:  [572.917, 517.765, 500.372, 493.708, 493.568, 487.073, 484.801]\n"
     ]
    }
   ],
   "source": [
    "clusters_arr=[2,3,4,5,6,7,8]\n",
    "methods=[\"kmeans\",\"hca\"]\n",
    "duration_arr=[]\n",
    "labels_arr=[]\n",
    "inertia_arr=[]\n",
    "silhouette_arr=[]\n",
    "ch_score_arr=[]\n",
    "datasets=[train_scaled, unique_scaled]\n",
    "\n",
    "for dataset in datasets:    \n",
    "    for method in methods:\n",
    "        duration_arr.clear()\n",
    "        labels_arr.clear()\n",
    "        inertia_arr.clear()\n",
    "        silhouette_arr.clear()\n",
    "        ch_score_arr.clear()\n",
    "        \n",
    "        for n_clusters in clusters_arr:\n",
    "            t_start=time.time()\n",
    "            labels,inertia,silhouette,ch_score=apply_clustering(dataset, n_clusters, method,0,0)\n",
    "            t_end=time.time()\n",
    "\n",
    "            duration_arr.append(round(t_end-t_start,3))\n",
    "            labels_arr.append(labels)\n",
    "            if inertia is None:\n",
    "                inertia_arr.append(inertia)\n",
    "            else:\n",
    "                inertia_arr.append(round(inertia,3))\n",
    "            silhouette_arr.append(round(silhouette,3))\n",
    "            ch_score_arr.append(round(ch_score,3))\n",
    "\n",
    "        print(\"\\nMethod: \", method)\n",
    "        print(\"Clusters:\", clusters_arr)\n",
    "        print(\"Duration: \", duration_arr)\n",
    "        print(\"Labels: \",labels_arr)\n",
    "        print(\"Inertia: \",inertia_arr)\n",
    "        print(\"Silhouette score: \",silhouette_arr)\n",
    "        print(\"Calinski-Harabasz score: \",ch_score_arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b721fd",
   "metadata": {},
   "source": [
    "Though HCA takes longer than kmeans (it is a much complex statistical approach), it apparently gives better results for both datasets with higher scores. Bear in mind that for HCA we only used euclidean distance. Linkage was tested for the best approach.\n",
    "\n",
    "For the TRAIN dataset considering kmeans and its resulting silhoutte scores, the best result points to a 2 clusters solution, however the difference in scores to other cluster solutions is not big. Nevertheless, CH score is clearly higher for a 2 cluster solution. When applying HCA the resulting CH score indicates that a 4 or 5 clusters solution fits data the best, when considering average, complete or Ward's linkage.\n",
    "\n",
    "On the other hand, UNIQUE dataset always shows better results for a 2 or 3 cluster solution, when using kmeans. However, HCA, when applying average or complete linkage, gave back some highly suspicous silhouette score values (0.95) for every cluster solution. Ward's linkage seem to provide a more balanced solution, pointing to a 2 or 3 cluster solution as the best solution, with sillouette scores arround 0.70). \n",
    "\n",
    "This apparently indicates that both algorithms have some trouble providing a good solution. The datasets are probably too noisy. An approach to overcome this scenario could be using kmeans to have 2 clusters, and then apply HCA independently to each cluster and see the results.\n",
    "\n",
    "Results were a little bit different when using DBSCAN: we have searched for the best parameters, both eps and minimal samples, for both datasets. \n",
    "\n",
    "EPS is the maximum distance between two samples for one to be considered as in the neighborhood of the other and is the most important DBSCAN parameter to choose appropriately. The minimal number of samples in a neighborhood for a point to be considered as a core point is another important parameter.\n",
    "\n",
    "For TRAIN dataset eps until 11 give the best scores together with min_sample 15, returning a 2 cluster solution, with a score=0.55. For UNIQUE dataser eps=30 and min_sample=30 returns a 4 cluster solution, with a score=0.80. For higher scores, a 2 cluster solution is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aae723b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For TRAIN dataset:\n",
      "With eps= 11  and min_sample= 15\n",
      "Duration:  17.790655374526978\n",
      "Labels:  [0 0 0 ... 0 0 0]\n",
      "Number of Clusters:  2\n",
      "Silhouette score:  0.552460766781026\n",
      "CH score:  214.5243814518202\n"
     ]
    }
   ],
   "source": [
    "#Apply different DBSCAN models to train dataset\n",
    "eps_arr=11\n",
    "min_samples_arr=15\n",
    "print(\"For TRAIN dataset:\")\n",
    "t_start=time.time()                                                    \n",
    "labels,inertia,silhouette,ch_score=apply_clustering(train_scaled, 0, \"dbscan\",eps_arr,min_samples_arr) #melhor eps=11 e min_samples=30 (2clusters)\n",
    "t_end=time.time()  \n",
    "print(\"With eps=\",eps_arr,\" and min_sample=\",min_samples_arr)\n",
    "print(\"Duration: \", t_end-t_start)\n",
    "print(\"Labels: \",labels)\n",
    "print(\"Number of Clusters: \", len(np.unique(labels)))\n",
    "print(\"Silhouette score: \",silhouette)\n",
    "print(\"CH score: \",ch_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aed0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For TRAIN dataset:\n",
      "With eps= 30  and min_sample= 10\n",
      "Duration:  20.2274968624115\n",
      "Labels:  [0 0 0 ... 0 0 0]\n",
      "Number of Clusters:  4\n",
      "Silhouette score:  0.8066185975569313\n",
      "CH score:  305.0690514929589\n"
     ]
    }
   ],
   "source": [
    "#Apply different DBSCAN models to UNIQUE dataset\n",
    "eps_arr=30\n",
    "min_samples_arr=10\n",
    "print(\"For TRAIN dataset:\")\n",
    "t_start=time.time()                                                    \n",
    "labels,inertia,silhouette,ch_score=apply_clustering(unique_scaled, 0, \"dbscan\",eps_arr,min_samples_arr) #melhor eps=30 e min_samples=10 (2clusters)\n",
    "t_end=time.time()  \n",
    "print(\"With eps=\",eps_arr,\" and min_sample=\",min_samples_arr)\n",
    "print(\"Duration: \", t_end-t_start)\n",
    "print(\"Labels: \",labels)\n",
    "print(\"Number of Clusters: \", len(np.unique(labels)))\n",
    "print(\"Silhouette score: \",silhouette)\n",
    "print(\"CH score: \",ch_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701d68d",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd143697",
   "metadata": {},
   "source": [
    "## Objective 2: Evaluating clustering with extrinsic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "283d1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset\n",
    "kmdt = KMeans(n_clusters=2,n_init=10, random_state=0).fit(train_scaled)\n",
    "hcadt = AgglomerativeClustering(n_clusters=2, linkage=\"ward\").fit(train_scaled)\n",
    "dbscandt = DBSCAN(eps=11, min_samples=15).fit(train_scaled)\n",
    "#unique dataset\n",
    "kdum = KMeans(n_clusters=2,n_init=10, random_state=0).fit(unique_scaled)\n",
    "hcadum = AgglomerativeClustering(n_clusters=2, linkage=\"ward\").fit(unique_scaled)\n",
    "dbscandu = DBSCAN(eps=30, min_samples=10).fit(unique_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d53320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showClusteringResults(X, clusters, labels=None):\n",
    "    sil=silhouette_score(X, clusters)\n",
    "    ch=calinski_harabasz_score(X, clusters)\n",
    "    print(\"Silhouette score\", sil)\n",
    "    print(\"Calinski Harabasz score\", ch)\n",
    "    if labels is not None:\n",
    "        hom=homogeneity_score(clusters, labels)\n",
    "        cmp=completeness_score(clusters, labels)\n",
    "        vms=v_measure_score(clusters, labels)\n",
    "        print(\"Homogeneity score\", hom)    \n",
    "        print(\"Completeness score\", cmp)\n",
    "        print(\"V-measure score\", vms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968db257",
   "metadata": {},
   "source": [
    "### Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1f025db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering results from 2 clusters kmeans for Train dataset: \n",
      "Silhouette score 0.34653151281555533\n",
      "Calinski Harabasz score 9936.174442755846\n",
      "Homogeneity score 0.42815131138588053\n",
      "Completeness score 0.22511844630054353\n",
      "V-measure score 0.2950840961690294\n",
      "\n",
      "Clustering results from 2 clusters HCA for Train dataset: \n",
      "Silhouette score 0.33284350107579774\n",
      "Calinski Harabasz score 9125.608098749608\n",
      "Homogeneity score 0.39878967636158946\n",
      "Completeness score 0.21115377979858774\n",
      "V-measure score 0.2761106678265331\n",
      "\n",
      "Clustering results clusters DBSCAN for Train dataset: \n",
      "Silhouette score 0.5524607667809489\n",
      "Calinski Harabasz score 214.52438145182023\n",
      "Homogeneity score 0.04477098684262668\n",
      "Completeness score 0.00033809193642512494\n",
      "V-measure score 0.0006711158838524886\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClustering results from 2 clusters kmeans for Train dataset: \")\n",
    "showClusteringResults(train_scaled, kmdt.labels_, y_classes)\n",
    "print(\"\\nClustering results from 2 clusters HCA for Train dataset: \")\n",
    "showClusteringResults(train_scaled, hcadt.labels_, y_classes)\n",
    "print(\"\\nClustering results clusters DBSCAN for Train dataset: \")\n",
    "showClusteringResults(train_scaled, dbscandt.labels_+1, y_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc83c00",
   "metadata": {},
   "source": [
    "### Unique Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8f16059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering results from 2 clusters kmeans for Unique dataset: \n",
      "Silhouette score 0.7005853361857551\n",
      "Calinski Harabasz score 572.9173211837198\n",
      "Homogeneity score 0.03822493667563354\n",
      "Completeness score 0.0006152216063172981\n",
      "V-measure score 0.0012109531980918117\n",
      "\n",
      "Clustering results from 2 clusters HCA for Unique dataset: \n",
      "Silhouette score 0.7005853361857551\n",
      "Calinski Harabasz score 572.9173211837198\n",
      "Homogeneity score 0.03822493667563354\n",
      "Completeness score 0.0006152216063172981\n",
      "V-measure score 0.0012109531980918117\n",
      "\n",
      "Clustering results clusters DBSCAN for Unique dataset: \n",
      "Silhouette score 0.8066185975569313\n",
      "Calinski Harabasz score 305.0690514929589\n",
      "Homogeneity score 0.12747222571847328\n",
      "Completeness score 0.0028036653234625977\n",
      "V-measure score 0.005486655375650999\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClustering results from 2 clusters kmeans for Unique dataset: \")\n",
    "showClusteringResults(unique_scaled, kdum.labels_, y_classes)\n",
    "print(\"\\nClustering results from 2 clusters HCA for Unique dataset: \")\n",
    "showClusteringResults(unique_scaled, hcadum.labels_, y_classes)\n",
    "print(\"\\nClustering results clusters DBSCAN for Unique dataset: \")\n",
    "showClusteringResults(unique_scaled, dbscandu.labels_+1, y_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311cca4",
   "metadata": {},
   "source": [
    "The present data has been classified in 5 classes according to its critical temperature.\n",
    "\n",
    "However, when trying to perform clustering this solution was not achieved by any of the proposed algorithms. The optimal state for all of these clustering techniques was around 2 clusters which goes away from our pre-known 5 classes. This goes away from what the truth is. We can see that reflected across the board in the extrinsic methods.\n",
    "\n",
    "According to the results, clustering both datasets using KMeans appears to be the best alternative. For the Train dataset, KMeans has the highest Silhouette score, an intrinsic method for assessing clustering quality, and the highest Calinski Harabasz score, another intrinsic method based on variance ratio. Furthermore, Homogeneity, Completeness, and V-measure scores—extrinsic techniques that assess the cluster's consistency with existing labels—show that KMeans performs better than HCA.\n",
    "\n",
    "Regarding the Unique dataset, the Silhouette, Calinski Harabasz, Homogeneity, Completeness, and V-measure scores for KMeans and HCA are the same. It is crucial to remember that the distribution of the Unique dataset differs from that of the Train dataset and that the same clustering algorithm may act differently depending on the dataset. Given the results, clustering the Unique dataset using KMeans is still a reasonable choice.\n",
    "\n",
    "Based on both intrinsic and extrinsic evaluation methodologies, KMeans emerges as the top candidate for clustering both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147be3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
