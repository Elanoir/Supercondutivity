{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f929686c",
      "metadata": {
        "id": "f929686c"
      },
      "source": [
        "The UCI Superconductivity Dataset has two files:\n",
        "\n",
        "(1) train.csv contains 81 features extracted from 21263 superconductors along with the critical temperature in the 82nd column\n",
        "\n",
        "(2) unique_m.csv contains the chemical formula broken up for all the 21263 superconductors from the train.csv file. The last two columns have the critical temperature and chemical formula. \n",
        "\n",
        "The goal was to predict the critical temperature based on the features extracted.\n",
        "\n",
        "### Task: Build Supervised Learning Models\n",
        "\n",
        "### Objectives\n",
        "1: Dimensionality Reduction\n",
        "\n",
        "2: Create a Regression and Classification model\n",
        "\n",
        "________________________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91c68e28",
      "metadata": {
        "id": "91c68e28"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import CategoricalNB, GaussianNB\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, accuracy_score, max_error, mean_absolute_error\n",
        "from sklearn.metrics import f1_score, confusion_matrix,matthews_corrcoef, precision_score, recall_score\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "Gn-w3Us3GOCi"
      },
      "id": "Gn-w3Us3GOCi"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "373a3c44",
      "metadata": {
        "id": "373a3c44"
      },
      "outputs": [],
      "source": [
        "#Load both base datasets\n",
        "data_train = pd.read_csv(\"train.csv\")\n",
        "data_unique = pd.read_csv('unique_m.csv')\n",
        "\n",
        "#ChatGPT generated conversion method but slightly adapted from original output\n",
        "def get_temp_class(temp):\n",
        "    if temp < 1.0:\n",
        "        return \"VeryLow\"\n",
        "    elif temp < 5.0:\n",
        "        return \"Low\"\n",
        "    elif temp < 20.0:\n",
        "        return \"Medium\"\n",
        "    elif temp < 100.0:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"VeryHigh\"\n",
        "\n",
        "    \n",
        "data_train['temp_class'] = data_train['critical_temp'].apply(get_temp_class)\n",
        "\n",
        "#Separate the dependent variable as our target. This value is the same in both datasets and same order\n",
        "y = data_train.values[:,81]\n",
        "y_classes = data_train.values[:,82]\n",
        "\n",
        "#Drop the dependent variable and the identifier variables\n",
        "data_train.drop(columns=[\"critical_temp\", \"temp_class\"], inplace=True)\n",
        "data_unique.drop(columns=[\"critical_temp\", \"material\"], inplace=True)\n",
        "\n",
        "#Create a dataset with both combined\n",
        "data_combined = pd.concat([data_train, data_unique], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective 1: Dimensionality reduction by applying SVD"
      ],
      "metadata": {
        "id": "G2HNlTruGTlU"
      },
      "id": "G2HNlTruGTlU"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "afccff4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afccff4d",
        "outputId": "801005a1-b629-4008-88c4-308fbfe839ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 1 components have a combined importance of  0.0507\n",
            "first 2 components have a combined importance of  0.0775\n",
            "first 3 components have a combined importance of  0.1032\n",
            "first 4 components have a combined importance of  0.1263\n",
            "first 5 components have a combined importance of  0.1467\n",
            "first 6 components have a combined importance of  0.1633\n",
            "first 7 components have a combined importance of  0.1795\n",
            "first 8 components have a combined importance of  0.1946\n",
            "first 9 components have a combined importance of  0.2087\n",
            "first 10 components have a combined importance of  0.2224\n",
            "first 11 components have a combined importance of  0.2353\n",
            "first 12 components have a combined importance of  0.2480\n",
            "first 13 components have a combined importance of  0.2602\n",
            "first 14 components have a combined importance of  0.2719\n",
            "first 15 components have a combined importance of  0.2837\n",
            "first 16 components have a combined importance of  0.2952\n",
            "first 17 components have a combined importance of  0.3066\n",
            "first 18 components have a combined importance of  0.3177\n",
            "first 19 components have a combined importance of  0.3287\n",
            "first 20 components have a combined importance of  0.3394\n",
            "first 21 components have a combined importance of  0.3500\n",
            "first 22 components have a combined importance of  0.3604\n",
            "first 23 components have a combined importance of  0.3707\n",
            "first 24 components have a combined importance of  0.3808\n",
            "first 25 components have a combined importance of  0.3908\n",
            "first 26 components have a combined importance of  0.4007\n",
            "first 27 components have a combined importance of  0.4105\n",
            "first 28 components have a combined importance of  0.4202\n",
            "first 29 components have a combined importance of  0.4298\n",
            "first 30 components have a combined importance of  0.4392\n",
            "first 31 components have a combined importance of  0.4486\n",
            "first 32 components have a combined importance of  0.4579\n",
            "first 33 components have a combined importance of  0.4671\n",
            "first 34 components have a combined importance of  0.4763\n",
            "first 35 components have a combined importance of  0.4855\n",
            "first 36 components have a combined importance of  0.4945\n",
            "first 37 components have a combined importance of  0.5035\n",
            "first 38 components have a combined importance of  0.5125\n",
            "first 39 components have a combined importance of  0.5215\n",
            "first 40 components have a combined importance of  0.5305\n",
            "first 41 components have a combined importance of  0.5394\n",
            "first 42 components have a combined importance of  0.5483\n",
            "first 43 components have a combined importance of  0.5571\n",
            "first 44 components have a combined importance of  0.5660\n",
            "first 45 components have a combined importance of  0.5748\n",
            "first 46 components have a combined importance of  0.5837\n",
            "first 47 components have a combined importance of  0.5925\n",
            "first 48 components have a combined importance of  0.6013\n",
            "first 49 components have a combined importance of  0.6101\n",
            "first 50 components have a combined importance of  0.6188\n",
            "first 51 components have a combined importance of  0.6275\n",
            "first 52 components have a combined importance of  0.6362\n",
            "first 53 components have a combined importance of  0.6449\n",
            "first 54 components have a combined importance of  0.6535\n",
            "first 55 components have a combined importance of  0.6621\n",
            "first 56 components have a combined importance of  0.6706\n",
            "first 57 components have a combined importance of  0.6791\n",
            "first 58 components have a combined importance of  0.6874\n",
            "first 59 components have a combined importance of  0.6958\n",
            "first 60 components have a combined importance of  0.7040\n",
            "first 61 components have a combined importance of  0.7122\n",
            "first 62 components have a combined importance of  0.7204\n",
            "first 63 components have a combined importance of  0.7284\n",
            "first 64 components have a combined importance of  0.7363\n",
            "first 65 components have a combined importance of  0.7442\n",
            "first 66 components have a combined importance of  0.7520\n",
            "first 67 components have a combined importance of  0.7597\n",
            "first 68 components have a combined importance of  0.7672\n",
            "first 69 components have a combined importance of  0.7748\n",
            "first 70 components have a combined importance of  0.7822\n",
            "first 71 components have a combined importance of  0.7896\n",
            "first 72 components have a combined importance of  0.7966\n",
            "first 73 components have a combined importance of  0.8036\n",
            "first 74 components have a combined importance of  0.8104\n",
            "first 75 components have a combined importance of  0.8172\n",
            "first 76 components have a combined importance of  0.8238\n",
            "first 77 components have a combined importance of  0.8302\n",
            "first 78 components have a combined importance of  0.8364\n",
            "first 79 components have a combined importance of  0.8427\n",
            "first 80 components have a combined importance of  0.8486\n",
            "first 81 components have a combined importance of  0.8546\n",
            "first 82 components have a combined importance of  0.8604\n",
            "first 83 components have a combined importance of  0.8660\n",
            "first 84 components have a combined importance of  0.8712\n",
            "first 85 components have a combined importance of  0.8765\n",
            "first 86 components have a combined importance of  0.8815\n",
            "first 87 components have a combined importance of  0.8865\n",
            "first 88 components have a combined importance of  0.8914\n",
            "first 89 components have a combined importance of  0.8962\n",
            "first 90 components have a combined importance of  0.9008\n",
            "first 91 components have a combined importance of  0.9053\n",
            "first 92 components have a combined importance of  0.9095\n",
            "first 93 components have a combined importance of  0.9135\n",
            "first 94 components have a combined importance of  0.9174\n",
            "first 95 components have a combined importance of  0.9212\n",
            "first 96 components have a combined importance of  0.9250\n",
            "first 97 components have a combined importance of  0.9287\n",
            "first 98 components have a combined importance of  0.9323\n",
            "first 99 components have a combined importance of  0.9354\n",
            "first 100 components have a combined importance of  0.9384\n",
            "first 101 components have a combined importance of  0.9414\n",
            "first 102 components have a combined importance of  0.9443\n",
            "first 103 components have a combined importance of  0.9470\n",
            "first 104 components have a combined importance of  0.9497\n",
            "first 105 components have a combined importance of  0.9522\n",
            "first 106 components have a combined importance of  0.9546\n",
            "first 107 components have a combined importance of  0.9568\n",
            "first 108 components have a combined importance of  0.9589\n",
            "first 109 components have a combined importance of  0.9611\n",
            "first 110 components have a combined importance of  0.9632\n",
            "first 111 components have a combined importance of  0.9651\n",
            "first 112 components have a combined importance of  0.9669\n",
            "first 113 components have a combined importance of  0.9686\n",
            "first 114 components have a combined importance of  0.9703\n",
            "first 115 components have a combined importance of  0.9719\n",
            "first 116 components have a combined importance of  0.9734\n",
            "first 117 components have a combined importance of  0.9749\n",
            "first 118 components have a combined importance of  0.9762\n",
            "first 119 components have a combined importance of  0.9774\n",
            "first 120 components have a combined importance of  0.9786\n",
            "first 121 components have a combined importance of  0.9798\n",
            "first 122 components have a combined importance of  0.9809\n",
            "first 123 components have a combined importance of  0.9819\n",
            "first 124 components have a combined importance of  0.9830\n",
            "first 125 components have a combined importance of  0.9840\n",
            "first 126 components have a combined importance of  0.9850\n",
            "first 127 components have a combined importance of  0.9860\n",
            "first 128 components have a combined importance of  0.9869\n",
            "first 129 components have a combined importance of  0.9878\n",
            "first 130 components have a combined importance of  0.9886\n",
            "first 131 components have a combined importance of  0.9894\n",
            "first 132 components have a combined importance of  0.9902\n",
            "first 133 components have a combined importance of  0.9909\n",
            "first 134 components have a combined importance of  0.9916\n",
            "first 135 components have a combined importance of  0.9922\n",
            "first 136 components have a combined importance of  0.9929\n",
            "first 137 components have a combined importance of  0.9935\n",
            "first 138 components have a combined importance of  0.9941\n",
            "first 139 components have a combined importance of  0.9947\n",
            "first 140 components have a combined importance of  0.9952\n",
            "first 141 components have a combined importance of  0.9957\n",
            "first 142 components have a combined importance of  0.9961\n",
            "first 143 components have a combined importance of  0.9966\n",
            "first 144 components have a combined importance of  0.9970\n",
            "first 145 components have a combined importance of  0.9974\n",
            "first 146 components have a combined importance of  0.9977\n",
            "first 147 components have a combined importance of  0.9980\n",
            "first 148 components have a combined importance of  0.9984\n",
            "first 149 components have a combined importance of  0.9987\n",
            "first 150 components have a combined importance of  0.9989\n",
            "first 151 components have a combined importance of  0.9992\n",
            "first 152 components have a combined importance of  0.9993\n",
            "first 153 components have a combined importance of  0.9995\n",
            "first 154 components have a combined importance of  0.9997\n",
            "first 155 components have a combined importance of  0.9998\n",
            "first 156 components have a combined importance of  0.9999\n",
            "first 157 components have a combined importance of  0.9999\n",
            "first 158 components have a combined importance of  1.0000\n",
            "first 159 components have a combined importance of  1.0000\n",
            "first 160 components have a combined importance of  1.0000\n",
            "first 161 components have a combined importance of  1.0000\n",
            "first 162 components have a combined importance of  1.0000\n",
            "first 163 components have a combined importance of  1.0000\n",
            "first 164 components have a combined importance of  1.0000\n",
            "first 165 components have a combined importance of  1.0000\n",
            "first 166 components have a combined importance of  1.0000\n",
            "first 167 components have a combined importance of  1.0000\n"
          ]
        }
      ],
      "source": [
        "#Objective 1 - Apply SVD\n",
        "#Apply Standard Scaler\n",
        "scaler_combined = StandardScaler().fit(data_combined.values)\n",
        "scaled_combined = scaler_combined.transform(data_combined.values)\n",
        "\n",
        "U, S, VT = np.linalg.svd(scaled_combined)\n",
        "\n",
        "for i in range(len(S)):\n",
        "  print(\"first %d components have a combined importance of %7.4f\" %(i+1, S[:i+1].sum()/S.sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ace709c",
      "metadata": {
        "id": "5ace709c"
      },
      "source": [
        "The Singular Value Decomposition (SVD) of a given matrix is a factorization of that matrix into three matrices (U, S and VT). It can be used to reduce the dimensionality of a dataset, i.e., the number of variables. This way it is possible to extract the more important variables in a dataset (or the ones that contibute more to explain the total variance of a given dataset). Dimension reduction is important because it can mean reducing the time to build and perform a model and score data.\n",
        "\n",
        "In this particular case, after SVD of the data considering 167 variables we can conclude that the first 140 components are sufficient to explain 100% of the total variance of the dataset. Moreover, the 73 first components are sufficient to explain 80% of the data, the first 80 components explain 85%, the first 90 components explain 90% of the data, and the first 105 components explain 95% of the data. \n",
        "\n",
        "Usually, retaining >85% of total explained variance of a dataset can be considered suficient to perfom data analysis, but it depends on what kind of analysis is going to be performed next. \n",
        "\n",
        "So, let's check how data behaves considering dimensionality reduction while building a supervised learning model, both using regression and classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective 2: Create a Regression and Classification model"
      ],
      "metadata": {
        "id": "hTpU8BfdGbT-"
      },
      "id": "hTpU8BfdGbT-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective 2.1: Regression Model using Linear Regression or Decision Tree regression\n"
      ],
      "metadata": {
        "id": "G3U5C6UzGgsM"
      },
      "id": "G3U5C6UzGgsM"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d759106c",
      "metadata": {
        "id": "d759106c"
      },
      "outputs": [],
      "source": [
        "#Objective 2.1\n",
        "X = scaled_combined\n",
        "# Split the input data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the regression model analysis, the goal is to predict the critical temperature of a given superconductor.\n",
        "\n",
        "We started by searching for the best hyperparameters for a Decision Tree Regressor model, considering max_depth and min_amples parameters."
      ],
      "metadata": {
        "id": "6j2p4oPeNxv6"
      },
      "id": "6j2p4oPeNxv6"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "0bebaeb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bebaeb8",
        "outputId": "5145077d-2a2c-4796-f2c5-5891fb1a4911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best RVE is:  0.8898664821186723 for i:  16  j:  19\n",
            "The best rmse is:  11.262595629776115 for i:  16  j:  19\n",
            "The min Maximum Error is:  77.88934343434346 for i:  2  j:  3\n",
            "The min Mean Absolute Error is:  6.074043295935849 for i:  12  j:  19\n"
          ]
        }
      ],
      "source": [
        "rve = cs = irve = irmse = ics = ima = imar = jrmse = jrve = jma = jmar = 0\n",
        "ma = mar = rmse = 100000\n",
        "\n",
        "for i in range(2,20):\n",
        "    for j in range(1,20):\n",
        "        model_full= DecisionTreeRegressor(max_depth=j,min_samples_split=i,random_state=123).fit(X_train, y_train)\n",
        "        preds=model_full.predict(X_test)\n",
        "        explained_variance_score(y_test, preds)\n",
        "        corr, pval=pearsonr(y_test, preds)\n",
        "\n",
        "\n",
        "        if(explained_variance_score(y_test, preds)>rve):\n",
        "            rve = explained_variance_score(y_test, preds)\n",
        "            irve = i\n",
        "            jrve = j\n",
        "        if(mean_squared_error(y_test, preds, squared=False)<rmse):\n",
        "            rmse =  mean_squared_error(y_test, preds, squared=False)\n",
        "            irmse = i\n",
        "            jrmse = j\n",
        "        if(max_error(y_test, preds)<ma):\n",
        "            ma = max_error(y_test, preds)\n",
        "            ima = i\n",
        "            jma = j\n",
        "        if(mean_absolute_error(y_test, preds)<mar):\n",
        "            mar = mean_absolute_error(y_test, preds)\n",
        "            imar = i\n",
        "            jmar = j\n",
        "            \n",
        "            \n",
        "print(\"The best RVE is: \", rve , \"for i: \", irve, ' j: ', jrve)\n",
        "print(\"The best rmse is: \", rmse , \"for i: \", irmse, ' j: ', jrmse)\n",
        "print(\"The min Maximum Error is: \", ma , \"for i: \", ima, ' j: ', jma)\n",
        "print(\"The min Mean Absolute Error is: \", mar , \"for i: \", imar, ' j: ', jmar)            "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best results were obtained considering max_depth=16 and min_samples=19.\n",
        "\n",
        "The Decision Tree model considering these hyperparameters was applied to the entired dataset.\n",
        "\n",
        "Decision Trees tend to overfit and to mitigate that, a K-fold cross validation was implemented."
      ],
      "metadata": {
        "id": "HNtlidT2NwJj"
      },
      "id": "HNtlidT2NwJj"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6463e448",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6463e448",
        "outputId": "5cc51a9a-b064-4046-ea1f-5b2c31291240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RVE is:  0.8858403464424475\n",
            "The rmse is:  11.573429389916896\n",
            "The Correlation Score is is: 0.9418 (p-value=0.000000e+00)\n",
            "\n",
            "The Maximum Error is is:  181.4\n",
            "The Mean Absolute Error is:  6.440566676770679\n"
          ]
        }
      ],
      "source": [
        "#Full Dataset\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "TRUTH_nfold=None\n",
        "PREDS_nfold=None\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    mdl = DecisionTreeRegressor(max_depth=16,min_samples_split=19, random_state=123)\n",
        "    mdl.fit(X_train, y_train)\n",
        "    preds = mdl.predict(X_test)\n",
        "    if TRUTH_nfold is None:\n",
        "        PREDS_nfold=preds\n",
        "        TRUTH_nfold=y_test\n",
        "    else:\n",
        "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
        "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_test))\n",
        "\n",
        "print(\"The RVE is: \", explained_variance_score(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"The rmse is: \", mean_squared_error(TRUTH_nfold, PREDS_nfold, squared=False))\n",
        "corr, pval=pearsonr(TRUTH_nfold, PREDS_nfold)\n",
        "print(\"The Correlation Score is is: %6.4f (p-value=%e)\\n\"%(corr,pval))\n",
        "print(\"The Maximum Error is is: \", max_error(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"The Mean Absolute Error is: \", mean_absolute_error(TRUTH_nfold, PREDS_nfold))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A search on the best number (K) of components extracted from SVD to have a good model was performed. \n",
        "\n",
        "Accordingly to results obtained in Objective 1, the first 30 to 140 combined components were considered."
      ],
      "metadata": {
        "id": "vF2ApMZLNVNS"
      },
      "id": "vF2ApMZLNVNS"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "d58ad086",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d58ad086",
        "outputId": "9d9b65f2-2158-47e2-e5ca-7a1359f9e174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K:  65  with score:  0.8616789428863798\n"
          ]
        }
      ],
      "source": [
        "#Compute the SVD of the training data\n",
        "U, S, VT = np.linalg.svd(X_train)\n",
        "\n",
        "best_score = -1\n",
        "best_k = 0\n",
        "\n",
        "#Test between the usage of 30 to 140 components\n",
        "for k in range(30,140):\n",
        "    VT_k = VT[:k, :]\n",
        "    \n",
        "    #Use the reduced-dimensional training data to train a linear regression model\n",
        "    X_train_reduced = np.dot(X_train, VT_k.T)\n",
        "    reg_model_reduced = DecisionTreeRegressor(random_state=123).fit(X_train_reduced, y_train)\n",
        "    \n",
        "    #Compute the reduced-dimensional test data using the same k principal components\n",
        "    X_test_reduced = np.dot(X_test, VT_k.T)\n",
        "    \n",
        "    #Evaluate the performance of the model on the test data\n",
        "    score = reg_model_reduced.score(X_test_reduced, y_test)\n",
        "\n",
        "    if(score > best_score):\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "    \n",
        "print(\"Best K: \", best_k, \" with score: \", best_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree Regression model (considering the hyperparameters computed before) was applied to the dimensionality reduced data considering the first 65 components extracted by SVD from the data."
      ],
      "metadata": {
        "id": "FQdZdSGpPumW"
      },
      "id": "FQdZdSGpPumW"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d5afc204",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5afc204",
        "outputId": "628376c5-f0a3-41cd-ccfb-55f631bfa4d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The RVE is:  0.8642947946373474\n",
            "The rmse is:  12.618554316049945\n",
            "The Correlation Score is is: 0.9306 (p-value=0.000000e+00)\n",
            "\n",
            "The Maximum Error is is:  125.41333333333333\n",
            "The Mean Absolute Error is:  7.035567528780746\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "TRUTH_nfold=None\n",
        "PREDS_nfold=None\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    \n",
        "    U, S, VT = np.linalg.svd(X_train)\n",
        "    VT_k = VT[:65, :]\n",
        "    \n",
        "    X_train_reduced = np.dot(X_train, VT_k.T)\n",
        "    X_test_reduced = np.dot(X_test, VT_k.T)\n",
        "    \n",
        "    mdl = DecisionTreeRegressor(max_depth=16,min_samples_split=19,random_state=123)\n",
        "    mdl.fit(X_train_reduced, y_train)\n",
        "    preds = mdl.predict(X_test_reduced)\n",
        "    if TRUTH_nfold is None:\n",
        "        PREDS_nfold=preds\n",
        "        TRUTH_nfold=y_test\n",
        "    else:\n",
        "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
        "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_test))\n",
        "\n",
        "corr, pval=pearsonr(TRUTH_nfold, PREDS_nfold)\n",
        "\n",
        "print(\"The RVE is: \", explained_variance_score(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"The rmse is: \", mean_squared_error(TRUTH_nfold, PREDS_nfold, squared=False))\n",
        "print(\"The Correlation Score is is: %6.4f (p-value=%e)\\n\"%(corr,pval))\n",
        "print(\"The Maximum Error is is: \", max_error(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"The Mean Absolute Error is: \", mean_absolute_error(TRUTH_nfold, PREDS_nfold))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c49a683c",
      "metadata": {
        "id": "c49a683c"
      },
      "source": [
        "A Decision Tree Regressor model for the all dataset (167 variables) has a RVE of 0.886, while performing a dimensionality reduction and using only the first 65 components a RVE of 0.864 can be achieved with the same model.\n",
        "\n",
        "This indicates that the model can still have a good performance with much less variable information (about 1/3 of the information).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective 2.2: Classification Model using Decision Tree Classification and Naive Bayes"
      ],
      "metadata": {
        "id": "2XDUfrW4_m2v"
      },
      "id": "2XDUfrW4_m2v"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2f713124",
      "metadata": {
        "id": "2f713124"
      },
      "outputs": [],
      "source": [
        "#Objective 2.2\n",
        "X = scaled_combined\n",
        "# Split the input data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_classes, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the classification model analysis a similar approach was followed.\n",
        "\n",
        "The goal is to predict the class of a given superconductor regarding 5 classes of their critical temperature (very low, low, medium, high and very high).\n",
        "\n",
        "The best hyperparameters for a Decision Tree Classification model were investigated.\n",
        "\n",
        "This was not necessary regarding Gaussian Naive Bayes algorithm."
      ],
      "metadata": {
        "id": "6hnch81BLBiN"
      },
      "id": "6hnch81BLBiN"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f06bda9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f06bda9e",
        "outputId": "08d371d8-3399-4d3b-f670-92647979fa04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Precision :  0.8407690961786045 for (max_depth, min_samples_split):  [[19, 19]]\n",
            "Best Recall :  0.8365859393369386 for (max_depth, min_samples_split):  [[19, 19]]\n",
            "Best F1 Score :  0.837668835118235 for (max_depth, min_samples_split):  [[19, 19]]\n",
            "Best Matthews Correlation :  0.760957900520992 for (max_depth, min_samples_split):  [[19, 19]]\n",
            "Best Accuracy :  0.7275083769126107 for (max_depth, min_samples_split):  [[19, 19]]\n"
          ]
        }
      ],
      "source": [
        "#Test Params for Decision Tree Class\n",
        "iPre = iRe = iF1 = iMath = iAcc = [0]\n",
        "bestPre = bestRe = bestF1 = bestMath = bestAcc = 0\n",
        "\n",
        "for i in range(3,20):\n",
        "    for j in range(5,20):\n",
        "        model_full= DecisionTreeClassifier(max_depth=j,min_samples_split=i, class_weight='balanced',random_state=123).fit(X_train, y_train)\n",
        "        preds=model_full.predict(X_test)\n",
        "\n",
        "\n",
        "        if(precision_score(y_test, preds, average='weighted')>bestPre):\n",
        "            bestPre = precision_score(y_test, preds, average='weighted')\n",
        "            iPre[0] = ([j,i])\n",
        "        if(recall_score(y_test, preds, average='weighted')>bestRe):\n",
        "            bestRe =  recall_score(y_test, preds, average='weighted')\n",
        "            iRe[0] = ([j,i])\n",
        "        if(f1_score(y_test, preds, average='weighted')>bestF1):\n",
        "            bestF1 = f1_score(y_test, preds, average='weighted')\n",
        "            iF1[0] = ([j,i])\n",
        "        if(matthews_corrcoef(y_test, preds)>bestMath):\n",
        "            bestMath = matthews_corrcoef(y_test, preds)\n",
        "            iMath[0] = ([j,i])\n",
        "        if(accuracy_score(y_test, preds)>bestAcc):\n",
        "            bestAcc = matthews_corrcoef(y_test, preds)\n",
        "            iMath[0] = ([j,i])\n",
        "            \n",
        "            \n",
        "print(\"Best Precision : \" , bestPre, \"for (max_depth, min_samples_split): \", iPre)\n",
        "print(\"Best Recall : \" , bestRe, \"for (max_depth, min_samples_split): \", iRe)\n",
        "print(\"Best F1 Score : \" , bestF1, \"for (max_depth, min_samples_split): \", iF1)\n",
        "print(\"Best Matthews Correlation : \" , bestMath, \"for (max_depth, min_samples_split): \", iMath) \n",
        "print(\"Best Accuracy : \" , bestAcc, \"for (max_depth, min_samples_split): \", iAcc) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best results were obtained considering max_depth=19 and min_samples=19.\n",
        "\n",
        "A Decision Tree model considering these hyperparameters was applied to the entired dataset."
      ],
      "metadata": {
        "id": "DmaGXEI8OxoO"
      },
      "id": "DmaGXEI8OxoO"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d27ebb18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d27ebb18",
        "outputId": "d3fc961c-f7dd-4b99-fd52-f12393cd0816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision :  0.8217176516808238\n",
            "Recall :  0.8106570098292809\n",
            "F1 Score :  0.8140121031432227\n",
            "Matthews Correlation :  0.7262292946379665\n",
            "Accuracy :  0.8106570098292809\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "TRUTH_nfold=None\n",
        "PREDS_nfold=None\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_classes[train_index], y_classes[test_index]\n",
        "    \n",
        "    mdl = DecisionTreeClassifier(max_depth=19,min_samples_split=19,class_weight='balanced',random_state=123)\n",
        "    mdl.fit(X_train, y_train)\n",
        "    preds = mdl.predict(X_test)\n",
        "    if TRUTH_nfold is None:\n",
        "        PREDS_nfold=preds\n",
        "        TRUTH_nfold=y_test\n",
        "    else:\n",
        "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
        "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_test))\n",
        "    \n",
        "print(\"Precision : \" , precision_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Recall : \" , recall_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"F1 Score : \" , f1_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Matthews Correlation : \" , matthews_corrcoef(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"Accuracy : \" , accuracy_score(TRUTH_nfold, PREDS_nfold))  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classification model (considering the hyperparameters computed before) was applied to the dimensionality reduced data (K=65)."
      ],
      "metadata": {
        "id": "srsfavmvPXkD"
      },
      "id": "srsfavmvPXkD"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "68a29036",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68a29036",
        "outputId": "67a11178-f56f-456d-f6a2-cf0a646ab6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision :  0.805306717583287\n",
            "Recall :  0.7933029205662419\n",
            "F1 Score :  0.7973855867880227\n",
            "Matthews Correlation :  0.7008306366919892\n",
            "Accuracy :  0.7933029205662419\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "TRUTH_nfold=None\n",
        "PREDS_nfold=None\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_classes[train_index], y_classes[test_index]\n",
        "    \n",
        "    U, S, VT = np.linalg.svd(X_train)\n",
        "    VT_k = VT[:65, :]\n",
        "    \n",
        "    X_train_reduced = np.dot(X_train, VT_k.T)\n",
        "    X_test_reduced = np.dot(X_test, VT_k.T)\n",
        "    \n",
        "    mdl = DecisionTreeClassifier(max_depth=19,min_samples_split=19,class_weight='balanced', random_state=123)\n",
        "    mdl.fit(X_train_reduced, y_train)\n",
        "    preds = mdl.predict(X_test_reduced)\n",
        "    if TRUTH_nfold is None:\n",
        "        PREDS_nfold=preds\n",
        "        TRUTH_nfold=y_test\n",
        "    else:\n",
        "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
        "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_test))\n",
        "        \n",
        "print(\"Precision : \" , precision_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Recall : \" , recall_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"F1 Score : \" , f1_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Matthews Correlation : \" , matthews_corrcoef(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"Accuracy : \" , accuracy_score(TRUTH_nfold, PREDS_nfold))     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the classification models, the Decision Tree Classification model using the entire data set showed:\n",
        "\n",
        "a precision of around 0.822, a Matthews Correlation around 0.726 and accuracy of 0.811;\n",
        "\n",
        "while using the reduced data of 65 components the results are quite similar:\n",
        "\n",
        "the precision is 0.805, a Matthews Correlation around 0.700 and accuracy of 0.793,\n",
        "\n",
        "indicating that the model still have a good performance with much less information.\n",
        "\n"
      ],
      "metadata": {
        "id": "BBA-VKwn5gAi"
      },
      "id": "BBA-VKwn5gAi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Gaussian Naive Bayes model was also applied to the entired dataset."
      ],
      "metadata": {
        "id": "uEMnLDll3G3w"
      },
      "id": "uEMnLDll3G3w"
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "TRUTH_nfold=None\n",
        "PREDS_nfold=None\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_classes[train_index], y_classes[test_index]\n",
        "    \n",
        "    mdl = GaussianNB()\n",
        "    mdl.fit(X_train, y_train)\n",
        "    preds = mdl.predict(X_test)\n",
        "    if TRUTH_nfold is None:\n",
        "        PREDS_nfold=preds\n",
        "        TRUTH_nfold=y_test\n",
        "    else:\n",
        "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
        "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_test))\n",
        "    \n",
        "print(\"Precision : \" , precision_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Recall : \" , recall_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"F1 Score : \" , f1_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Matthews Correlation : \" , matthews_corrcoef(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"Accuracy : \" , accuracy_score(TRUTH_nfold, PREDS_nfold))\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjtATS1Q3GnQ",
        "outputId": "d59c8420-6f89-422f-ded4-187b4729e46b"
      },
      "id": "AjtATS1Q3GnQ",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision :  0.6149718717908375\n",
            "Recall :  0.3755349668438132\n",
            "F1 Score :  0.4027709317927835\n",
            "Matthews Correlation :  0.2560469722805726\n",
            "Accuracy :  0.3755349668438132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Naive Bayes model was also applied to the dimensionality reduced data (K=65)."
      ],
      "metadata": {
        "id": "OfIJrETvQu5l"
      },
      "id": "OfIJrETvQu5l"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5cab42dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cab42dd",
        "outputId": "83f04874-ba87-45df-a244-e30c6215645f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision :  0.4557740817150485\n",
            "Recall :  0.47655551897662607\n",
            "F1 Score :  0.4395545545226367\n",
            "Matthews Correlation :  0.19980652248165384\n",
            "Accuracy :  0.47655551897662607\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "TRUTH_nfold=None\n",
        "PREDS_nfold=None\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y_classes[train_index], y_classes[test_index]\n",
        "    \n",
        "    U, S, VT = np.linalg.svd(X_train)\n",
        "    VT_k = VT[:65, :]\n",
        "    \n",
        "    X_train_reduced = np.dot(X_train, VT_k.T)\n",
        "    X_test_reduced = np.dot(X_test, VT_k.T)\n",
        "    \n",
        "    mdl = GaussianNB()\n",
        "    mdl.fit(X_train_reduced, y_train)\n",
        "    preds = mdl.predict(X_test_reduced)\n",
        "    if TRUTH_nfold is None:\n",
        "        PREDS_nfold=preds\n",
        "        TRUTH_nfold=y_test\n",
        "    else:\n",
        "        PREDS_nfold=np.hstack((PREDS_nfold, preds))\n",
        "        TRUTH_nfold=np.hstack((TRUTH_nfold, y_test))\n",
        "        \n",
        "print(\"Precision : \" , precision_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Recall : \" , recall_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"F1 Score : \" , f1_score(TRUTH_nfold, PREDS_nfold, average='weighted'))\n",
        "print(\"Matthews Correlation : \" , matthews_corrcoef(TRUTH_nfold, PREDS_nfold))\n",
        "print(\"Accuracy : \" , accuracy_score(TRUTH_nfold, PREDS_nfold))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5239cd7f",
      "metadata": {
        "id": "5239cd7f"
      },
      "source": [
        "\n",
        "The Gaussian Naive Bayes model, however, shows a poorer performance compared to Decision Trees, \n",
        "\n",
        "even when using the entire dataset information, having:\n",
        "\n",
        "a precision of 0.615, a Matthews Correlation of 0.256 and accuracy of 0.376.\n",
        "\n",
        "When using dimensionality reduced data the scores get lower with:\n",
        "\n",
        "a precision around 0.456, a Matthews Correlation of  0.200, but a higher accuracy of 0.477.\n",
        "\n",
        "\n",
        "Naive Bayes makes the assumption that the variables are independent, which might be the cause for the decreasing in its precision in this case, compared to decision trees.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In the present assignment we were asked to build a Regression and a Classification Supervised Learning models to predict the critical temperature of some superconductores based on some of their features.\n",
        "\n",
        "Those features were contained in two files: (i) train.csv and (2) unique_m.csv.\n",
        "\n",
        "We decided to combined the two datasets in an attempt to extract the most important features to further build the SL models.\n",
        "\n",
        "\n",
        "A SVD analysis indicated that the first 65 extracted components were sufficient to have a good Decision Trees, both for Regression and Classification models, having scores quite similar to the model containing the entire data. \n",
        "\n",
        "Gaussian Bayes used for classification seemed to perform worst compared to Decision Trees. That migh be due to variable dependency.\n",
        "\n",
        "We have also used the train.csv data only to test the regression and classification models. \n",
        "The results were actualy quite similar and using the first 55 components extracted by SVD, a Decidion Tree Regression model with a RVE of 0.8555 can be built. \n",
        "\n",
        "When using the Decision Tree Classificator, the models showed a RVE of 0.822 for the entire dataset, and of 0.817 when using the first 55 components extracted by SVD from the 81 variables dataset.\n",
        "\n",
        "Howewer, the Gaussian Naive Bayes model has shown to perform better when using the first 55 components extrated by SVD from the train dataset, with a precision of 0.634 and a increased accuracy of 0.626, compared to the the combined dataset results. This might be due to the dependency of some varibles in study. Thus eliminating some of those noisy varibles might be favourable for the model.\n",
        "\n",
        "These results illustrates the importance of feature extraction using different SL models. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v3PcL2kDUX9n"
      },
      "id": "v3PcL2kDUX9n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
